{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKEbV/b5WtV8rmdO+eQNSt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","\n","import torch\n","import torchvision\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","from torchvision.datasets import MNIST\n","import matplotlib.pyplot as plt"],"metadata":{"id":"DvK5TaG5gyKM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Impostiamo gli hyperparametri\n","num_epochs = 100\n","batch_size = 128\n","learning_rate = 1e-3"],"metadata":{"id":"PU7pHSxFg5ii"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Trasformazioni delle immagini\n","def to_img(x):\n","    x = x.clamp(0, 1)\n","    x = x.view(x.size(0), 1, 28, 28)\n","    return x\n","\n","img_transform = transforms.Compose([\n","    transforms.ToTensor()\n","    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])"],"metadata":{"id":"Ui4dn1ryg_jW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Scarichiamo e prepariamo il dataset\n","dataset = MNIST('./data', transform=img_transform, download=True)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtlnMYHIhDZ8","executionInfo":{"status":"ok","timestamp":1700329795631,"user_tz":-60,"elapsed":2300,"user":{"displayName":"Eleonora Grassucci","userId":"15233023825143080101"}},"outputId":"5516402e-8f7b-401b-8aac-377b193dbf2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 82808265.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 22757034.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 18859807.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 3696978.22it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["if not os.path.exists('./vae_img'):\n","    os.mkdir('./vae_img')\n","\n","# Definiamo il modello\n","\n","class VAE(nn.Module):\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","\n","        self.fc1 = nn.Linear(784, 400)\n","        self.fc21 = nn.Linear(400, 20)\n","        self.fc22 = nn.Linear(400, 20)\n","        self.fc3 = nn.Linear(20, 400)\n","        self.fc4 = nn.Linear(400, 784)\n","\n","    def encode(self, x):\n","        h1 = F.relu(self.fc1(x))\n","        return self.fc21(h1), self.fc22(h1)\n","\n","    # Reparameterization trick del VAE: aggiustiamo un campione di una\n","    # distribuzione Normale (eps) con media (mu) e variance (std) imparate\n","    # dai dati tramite l'encoder\n","    def reparametrize(self, mu, logvar):\n","        std = logvar.mul(0.5).exp_()\n","        if torch.cuda.is_available():\n","            eps = torch.cuda.FloatTensor(std.size()).normal_()\n","        else:\n","            eps = torch.FloatTensor(std.size()).normal_()\n","        return eps.mul(std).add_(mu)\n","\n","    def decode(self, z):\n","        h3 = F.relu(self.fc3(z))\n","        return F.sigmoid(self.fc4(h3))\n","\n","    def forward(self, x):\n","        # Impariamo media e varianza tramite l'encoder\n","        mu, logvar = self.encode(x)\n","        # Reparameterization trick\n","        z = self.reparametrize(mu, logvar)\n","        # Facciamo il decoder per ricostruire l'immagine\n","        return self.decode(z), mu, logvar"],"metadata":{"id":"wpVJ0APGjIns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Definiamo un'istanza del modello, spostiamo su GPU\n","model = VAE()\n","if torch.cuda.is_available():\n","    model.cuda()\n","\n","# Definiamo la loss di ricostruzione\n","reconstruction_function = nn.MSELoss(size_average=False)\n","\n","# Combiniamo la loss di ricostruzione con la divergenza KL per costruire\n","# la loss finale del nostro VAE\n","def loss_function(recon_x, x, mu, logvar):\n","    \"\"\"\n","    recon_x: immagine ricostruita\n","    x: immagine originale\n","    mu: media\n","    logvar: varianza (log)\n","    \"\"\"\n","    recon_loss = reconstruction_function(recon_x, x)  # mse loss\n","    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n","    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n","    KLD = torch.sum(KLD_element).mul_(-0.5)\n","    # KL divergence\n","    return recon_loss + KLD\n","\n","# Definiamo l'ottimizzatore\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2pboteGUiLqe","executionInfo":{"status":"ok","timestamp":1700329796193,"user_tz":-60,"elapsed":7,"user":{"displayName":"Eleonora Grassucci","userId":"15233023825143080101"}},"outputId":"b36a82a1-a62c-43cb-bfb5-b032e7110661"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhJXlenVYBfk","executionInfo":{"status":"ok","timestamp":1700331702504,"user_tz":-60,"elapsed":1897044,"user":{"displayName":"Eleonora Grassucci","userId":"15233023825143080101"}},"outputId":"35736266-c435-4232-9ad8-3647c0b28dc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 0 [0/60000 (0%)]\tLoss: 170.775040\n","Train Epoch: 0 [12800/60000 (21%)]\tLoss: 48.476311\n","Train Epoch: 0 [25600/60000 (43%)]\tLoss: 41.478466\n","Train Epoch: 0 [38400/60000 (64%)]\tLoss: 40.038486\n","Train Epoch: 0 [51200/60000 (85%)]\tLoss: 37.196465\n","====> Epoch: 0 Average loss: 45.2655\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 36.354019\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 36.033051\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 35.309288\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 34.101967\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 33.377724\n","====> Epoch: 1 Average loss: 35.0659\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 32.880947\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 31.902874\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 33.770924\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 33.331100\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 33.899422\n","====> Epoch: 2 Average loss: 33.1507\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 32.566814\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 31.490595\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 32.370010\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 31.103035\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 31.702665\n","====> Epoch: 3 Average loss: 32.2515\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 31.675793\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 32.735218\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 32.242222\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 32.317627\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 31.143669\n","====> Epoch: 4 Average loss: 31.7557\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 31.863216\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 30.393167\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 31.455849\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 31.860157\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 31.361755\n","====> Epoch: 5 Average loss: 31.3470\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 32.656723\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 31.006073\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 31.723473\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 30.467043\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 30.195717\n","====> Epoch: 6 Average loss: 31.0969\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 30.592411\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 32.259869\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 30.389860\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 30.863882\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 30.619532\n","====> Epoch: 7 Average loss: 30.8976\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 31.180767\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 31.396481\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 31.321831\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 30.678230\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 31.167297\n","====> Epoch: 8 Average loss: 30.7506\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 31.168781\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 30.662674\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 30.272968\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 30.583485\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 31.389328\n","====> Epoch: 9 Average loss: 30.6050\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 29.941893\n","Train Epoch: 10 [12800/60000 (21%)]\tLoss: 30.671093\n","Train Epoch: 10 [25600/60000 (43%)]\tLoss: 29.647907\n","Train Epoch: 10 [38400/60000 (64%)]\tLoss: 29.803375\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 30.146250\n","====> Epoch: 10 Average loss: 30.4532\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 29.284653\n","Train Epoch: 11 [12800/60000 (21%)]\tLoss: 30.698662\n","Train Epoch: 11 [25600/60000 (43%)]\tLoss: 30.485809\n","Train Epoch: 11 [38400/60000 (64%)]\tLoss: 30.319984\n","Train Epoch: 11 [51200/60000 (85%)]\tLoss: 30.486568\n","====> Epoch: 11 Average loss: 30.3851\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 28.787693\n","Train Epoch: 12 [12800/60000 (21%)]\tLoss: 31.387600\n","Train Epoch: 12 [25600/60000 (43%)]\tLoss: 30.523994\n","Train Epoch: 12 [38400/60000 (64%)]\tLoss: 29.574608\n","Train Epoch: 12 [51200/60000 (85%)]\tLoss: 31.028061\n","====> Epoch: 12 Average loss: 30.2688\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 30.126640\n","Train Epoch: 13 [12800/60000 (21%)]\tLoss: 30.898594\n","Train Epoch: 13 [25600/60000 (43%)]\tLoss: 27.794754\n","Train Epoch: 13 [38400/60000 (64%)]\tLoss: 30.943533\n","Train Epoch: 13 [51200/60000 (85%)]\tLoss: 30.234207\n","====> Epoch: 13 Average loss: 30.1726\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 31.020325\n","Train Epoch: 14 [12800/60000 (21%)]\tLoss: 29.862663\n","Train Epoch: 14 [25600/60000 (43%)]\tLoss: 29.717239\n","Train Epoch: 14 [38400/60000 (64%)]\tLoss: 30.444407\n","Train Epoch: 14 [51200/60000 (85%)]\tLoss: 29.105400\n","====> Epoch: 14 Average loss: 30.0947\n","Train Epoch: 15 [0/60000 (0%)]\tLoss: 30.626984\n","Train Epoch: 15 [12800/60000 (21%)]\tLoss: 30.567284\n","Train Epoch: 15 [25600/60000 (43%)]\tLoss: 30.617878\n","Train Epoch: 15 [38400/60000 (64%)]\tLoss: 29.806200\n","Train Epoch: 15 [51200/60000 (85%)]\tLoss: 29.556559\n","====> Epoch: 15 Average loss: 30.0325\n","Train Epoch: 16 [0/60000 (0%)]\tLoss: 29.917107\n","Train Epoch: 16 [12800/60000 (21%)]\tLoss: 30.331226\n","Train Epoch: 16 [25600/60000 (43%)]\tLoss: 29.307346\n","Train Epoch: 16 [38400/60000 (64%)]\tLoss: 28.854002\n","Train Epoch: 16 [51200/60000 (85%)]\tLoss: 30.393221\n","====> Epoch: 16 Average loss: 29.9425\n","Train Epoch: 17 [0/60000 (0%)]\tLoss: 30.943428\n","Train Epoch: 17 [12800/60000 (21%)]\tLoss: 29.764324\n","Train Epoch: 17 [25600/60000 (43%)]\tLoss: 30.396776\n","Train Epoch: 17 [38400/60000 (64%)]\tLoss: 29.234753\n","Train Epoch: 17 [51200/60000 (85%)]\tLoss: 28.815323\n","====> Epoch: 17 Average loss: 29.8967\n","Train Epoch: 18 [0/60000 (0%)]\tLoss: 29.613083\n","Train Epoch: 18 [12800/60000 (21%)]\tLoss: 31.649290\n","Train Epoch: 18 [25600/60000 (43%)]\tLoss: 29.908356\n","Train Epoch: 18 [38400/60000 (64%)]\tLoss: 29.282997\n","Train Epoch: 18 [51200/60000 (85%)]\tLoss: 28.912710\n","====> Epoch: 18 Average loss: 29.8212\n","Train Epoch: 19 [0/60000 (0%)]\tLoss: 28.726562\n","Train Epoch: 19 [12800/60000 (21%)]\tLoss: 30.247217\n","Train Epoch: 19 [25600/60000 (43%)]\tLoss: 30.705448\n","Train Epoch: 19 [38400/60000 (64%)]\tLoss: 28.178875\n","Train Epoch: 19 [51200/60000 (85%)]\tLoss: 29.024334\n","====> Epoch: 19 Average loss: 29.7776\n","Train Epoch: 20 [0/60000 (0%)]\tLoss: 28.731323\n","Train Epoch: 20 [12800/60000 (21%)]\tLoss: 29.308260\n","Train Epoch: 20 [25600/60000 (43%)]\tLoss: 29.181019\n","Train Epoch: 20 [38400/60000 (64%)]\tLoss: 29.237064\n","Train Epoch: 20 [51200/60000 (85%)]\tLoss: 29.434170\n","====> Epoch: 20 Average loss: 29.7143\n","Train Epoch: 21 [0/60000 (0%)]\tLoss: 30.728748\n","Train Epoch: 21 [12800/60000 (21%)]\tLoss: 30.240263\n","Train Epoch: 21 [25600/60000 (43%)]\tLoss: 29.672600\n","Train Epoch: 21 [38400/60000 (64%)]\tLoss: 29.778816\n","Train Epoch: 21 [51200/60000 (85%)]\tLoss: 30.350163\n","====> Epoch: 21 Average loss: 29.6553\n","Train Epoch: 22 [0/60000 (0%)]\tLoss: 30.746151\n","Train Epoch: 22 [12800/60000 (21%)]\tLoss: 28.997921\n","Train Epoch: 22 [25600/60000 (43%)]\tLoss: 29.192974\n","Train Epoch: 22 [38400/60000 (64%)]\tLoss: 30.552494\n","Train Epoch: 22 [51200/60000 (85%)]\tLoss: 29.129341\n","====> Epoch: 22 Average loss: 29.5846\n","Train Epoch: 23 [0/60000 (0%)]\tLoss: 29.194992\n","Train Epoch: 23 [12800/60000 (21%)]\tLoss: 30.300497\n","Train Epoch: 23 [25600/60000 (43%)]\tLoss: 28.366184\n","Train Epoch: 23 [38400/60000 (64%)]\tLoss: 29.733847\n","Train Epoch: 23 [51200/60000 (85%)]\tLoss: 28.909300\n","====> Epoch: 23 Average loss: 29.5683\n","Train Epoch: 24 [0/60000 (0%)]\tLoss: 29.731564\n","Train Epoch: 24 [12800/60000 (21%)]\tLoss: 29.195091\n","Train Epoch: 24 [25600/60000 (43%)]\tLoss: 29.555912\n","Train Epoch: 24 [38400/60000 (64%)]\tLoss: 30.728054\n","Train Epoch: 24 [51200/60000 (85%)]\tLoss: 30.749184\n","====> Epoch: 24 Average loss: 29.5171\n","Train Epoch: 25 [0/60000 (0%)]\tLoss: 28.331734\n","Train Epoch: 25 [12800/60000 (21%)]\tLoss: 30.015184\n","Train Epoch: 25 [25600/60000 (43%)]\tLoss: 29.877613\n","Train Epoch: 25 [38400/60000 (64%)]\tLoss: 30.034714\n","Train Epoch: 25 [51200/60000 (85%)]\tLoss: 30.179857\n","====> Epoch: 25 Average loss: 29.4433\n","Train Epoch: 26 [0/60000 (0%)]\tLoss: 30.473869\n","Train Epoch: 26 [12800/60000 (21%)]\tLoss: 28.968548\n","Train Epoch: 26 [25600/60000 (43%)]\tLoss: 28.223057\n","Train Epoch: 26 [38400/60000 (64%)]\tLoss: 29.858198\n","Train Epoch: 26 [51200/60000 (85%)]\tLoss: 29.810595\n","====> Epoch: 26 Average loss: 29.3972\n","Train Epoch: 27 [0/60000 (0%)]\tLoss: 30.238400\n","Train Epoch: 27 [12800/60000 (21%)]\tLoss: 29.338699\n","Train Epoch: 27 [25600/60000 (43%)]\tLoss: 28.780304\n","Train Epoch: 27 [38400/60000 (64%)]\tLoss: 27.795788\n","Train Epoch: 27 [51200/60000 (85%)]\tLoss: 31.094891\n","====> Epoch: 27 Average loss: 29.3470\n","Train Epoch: 28 [0/60000 (0%)]\tLoss: 30.779072\n","Train Epoch: 28 [12800/60000 (21%)]\tLoss: 29.280746\n","Train Epoch: 28 [25600/60000 (43%)]\tLoss: 28.497257\n","Train Epoch: 28 [38400/60000 (64%)]\tLoss: 28.739288\n","Train Epoch: 28 [51200/60000 (85%)]\tLoss: 29.702318\n","====> Epoch: 28 Average loss: 29.3226\n","Train Epoch: 29 [0/60000 (0%)]\tLoss: 29.590275\n","Train Epoch: 29 [12800/60000 (21%)]\tLoss: 30.243935\n","Train Epoch: 29 [25600/60000 (43%)]\tLoss: 28.689606\n","Train Epoch: 29 [38400/60000 (64%)]\tLoss: 28.779293\n","Train Epoch: 29 [51200/60000 (85%)]\tLoss: 29.287018\n","====> Epoch: 29 Average loss: 29.2657\n","Train Epoch: 30 [0/60000 (0%)]\tLoss: 29.386639\n","Train Epoch: 30 [12800/60000 (21%)]\tLoss: 30.078247\n","Train Epoch: 30 [25600/60000 (43%)]\tLoss: 29.672041\n","Train Epoch: 30 [38400/60000 (64%)]\tLoss: 29.132097\n","Train Epoch: 30 [51200/60000 (85%)]\tLoss: 29.998280\n","====> Epoch: 30 Average loss: 29.2576\n","Train Epoch: 31 [0/60000 (0%)]\tLoss: 29.222202\n","Train Epoch: 31 [12800/60000 (21%)]\tLoss: 28.658512\n","Train Epoch: 31 [25600/60000 (43%)]\tLoss: 31.292217\n","Train Epoch: 31 [38400/60000 (64%)]\tLoss: 28.117449\n","Train Epoch: 31 [51200/60000 (85%)]\tLoss: 29.161543\n","====> Epoch: 31 Average loss: 29.2195\n","Train Epoch: 32 [0/60000 (0%)]\tLoss: 29.333847\n","Train Epoch: 32 [12800/60000 (21%)]\tLoss: 28.627075\n","Train Epoch: 32 [25600/60000 (43%)]\tLoss: 29.881809\n","Train Epoch: 32 [38400/60000 (64%)]\tLoss: 28.298086\n","Train Epoch: 32 [51200/60000 (85%)]\tLoss: 28.928305\n","====> Epoch: 32 Average loss: 29.1472\n","Train Epoch: 33 [0/60000 (0%)]\tLoss: 29.998314\n","Train Epoch: 33 [12800/60000 (21%)]\tLoss: 29.370449\n","Train Epoch: 33 [25600/60000 (43%)]\tLoss: 27.201099\n","Train Epoch: 33 [38400/60000 (64%)]\tLoss: 29.828354\n","Train Epoch: 33 [51200/60000 (85%)]\tLoss: 29.093971\n","====> Epoch: 33 Average loss: 29.1578\n","Train Epoch: 34 [0/60000 (0%)]\tLoss: 28.221619\n","Train Epoch: 34 [12800/60000 (21%)]\tLoss: 28.528650\n","Train Epoch: 34 [25600/60000 (43%)]\tLoss: 29.831326\n","Train Epoch: 34 [38400/60000 (64%)]\tLoss: 29.268154\n","Train Epoch: 34 [51200/60000 (85%)]\tLoss: 29.571371\n","====> Epoch: 34 Average loss: 29.1041\n","Train Epoch: 35 [0/60000 (0%)]\tLoss: 28.646294\n","Train Epoch: 35 [12800/60000 (21%)]\tLoss: 30.398247\n","Train Epoch: 35 [25600/60000 (43%)]\tLoss: 30.453392\n","Train Epoch: 35 [38400/60000 (64%)]\tLoss: 28.531868\n","Train Epoch: 35 [51200/60000 (85%)]\tLoss: 30.580019\n","====> Epoch: 35 Average loss: 29.0797\n","Train Epoch: 36 [0/60000 (0%)]\tLoss: 29.665255\n","Train Epoch: 36 [12800/60000 (21%)]\tLoss: 28.788641\n","Train Epoch: 36 [25600/60000 (43%)]\tLoss: 28.451885\n","Train Epoch: 36 [38400/60000 (64%)]\tLoss: 29.320084\n","Train Epoch: 36 [51200/60000 (85%)]\tLoss: 29.035616\n","====> Epoch: 36 Average loss: 29.0402\n","Train Epoch: 37 [0/60000 (0%)]\tLoss: 28.553711\n","Train Epoch: 37 [12800/60000 (21%)]\tLoss: 30.068264\n","Train Epoch: 37 [25600/60000 (43%)]\tLoss: 29.472866\n","Train Epoch: 37 [38400/60000 (64%)]\tLoss: 29.366318\n","Train Epoch: 37 [51200/60000 (85%)]\tLoss: 29.665110\n","====> Epoch: 37 Average loss: 28.9850\n","Train Epoch: 38 [0/60000 (0%)]\tLoss: 28.434366\n","Train Epoch: 38 [12800/60000 (21%)]\tLoss: 28.525763\n","Train Epoch: 38 [25600/60000 (43%)]\tLoss: 30.505531\n","Train Epoch: 38 [38400/60000 (64%)]\tLoss: 28.831776\n","Train Epoch: 38 [51200/60000 (85%)]\tLoss: 29.255733\n","====> Epoch: 38 Average loss: 29.0042\n","Train Epoch: 39 [0/60000 (0%)]\tLoss: 28.345612\n","Train Epoch: 39 [12800/60000 (21%)]\tLoss: 29.081184\n","Train Epoch: 39 [25600/60000 (43%)]\tLoss: 30.414865\n","Train Epoch: 39 [38400/60000 (64%)]\tLoss: 29.570601\n","Train Epoch: 39 [51200/60000 (85%)]\tLoss: 27.151674\n","====> Epoch: 39 Average loss: 28.9472\n","Train Epoch: 40 [0/60000 (0%)]\tLoss: 29.293331\n","Train Epoch: 40 [12800/60000 (21%)]\tLoss: 30.258223\n","Train Epoch: 40 [25600/60000 (43%)]\tLoss: 29.401028\n","Train Epoch: 40 [38400/60000 (64%)]\tLoss: 28.790936\n","Train Epoch: 40 [51200/60000 (85%)]\tLoss: 28.434828\n","====> Epoch: 40 Average loss: 28.9151\n","Train Epoch: 41 [0/60000 (0%)]\tLoss: 29.175823\n","Train Epoch: 41 [12800/60000 (21%)]\tLoss: 29.287809\n","Train Epoch: 41 [25600/60000 (43%)]\tLoss: 28.816605\n","Train Epoch: 41 [38400/60000 (64%)]\tLoss: 29.673981\n","Train Epoch: 41 [51200/60000 (85%)]\tLoss: 29.663017\n","====> Epoch: 41 Average loss: 28.9218\n","Train Epoch: 42 [0/60000 (0%)]\tLoss: 28.334949\n","Train Epoch: 42 [12800/60000 (21%)]\tLoss: 30.532974\n","Train Epoch: 42 [25600/60000 (43%)]\tLoss: 29.906006\n","Train Epoch: 42 [38400/60000 (64%)]\tLoss: 29.113708\n","Train Epoch: 42 [51200/60000 (85%)]\tLoss: 29.694988\n","====> Epoch: 42 Average loss: 28.8929\n","Train Epoch: 43 [0/60000 (0%)]\tLoss: 28.064705\n","Train Epoch: 43 [12800/60000 (21%)]\tLoss: 28.830181\n","Train Epoch: 43 [25600/60000 (43%)]\tLoss: 28.512135\n","Train Epoch: 43 [38400/60000 (64%)]\tLoss: 29.139832\n","Train Epoch: 43 [51200/60000 (85%)]\tLoss: 28.845036\n","====> Epoch: 43 Average loss: 28.8667\n","Train Epoch: 44 [0/60000 (0%)]\tLoss: 29.213339\n","Train Epoch: 44 [12800/60000 (21%)]\tLoss: 27.600090\n","Train Epoch: 44 [25600/60000 (43%)]\tLoss: 29.296480\n","Train Epoch: 44 [38400/60000 (64%)]\tLoss: 28.099056\n","Train Epoch: 44 [51200/60000 (85%)]\tLoss: 29.534323\n","====> Epoch: 44 Average loss: 28.8249\n","Train Epoch: 45 [0/60000 (0%)]\tLoss: 28.124754\n","Train Epoch: 45 [12800/60000 (21%)]\tLoss: 29.306747\n","Train Epoch: 45 [25600/60000 (43%)]\tLoss: 29.632227\n","Train Epoch: 45 [38400/60000 (64%)]\tLoss: 27.470188\n","Train Epoch: 45 [51200/60000 (85%)]\tLoss: 28.070513\n","====> Epoch: 45 Average loss: 28.8206\n","Train Epoch: 46 [0/60000 (0%)]\tLoss: 28.877884\n","Train Epoch: 46 [12800/60000 (21%)]\tLoss: 27.356880\n","Train Epoch: 46 [25600/60000 (43%)]\tLoss: 28.411934\n","Train Epoch: 46 [38400/60000 (64%)]\tLoss: 28.321430\n","Train Epoch: 46 [51200/60000 (85%)]\tLoss: 29.146172\n","====> Epoch: 46 Average loss: 28.8167\n","Train Epoch: 47 [0/60000 (0%)]\tLoss: 29.257593\n","Train Epoch: 47 [12800/60000 (21%)]\tLoss: 28.546116\n","Train Epoch: 47 [25600/60000 (43%)]\tLoss: 26.913448\n","Train Epoch: 47 [38400/60000 (64%)]\tLoss: 28.765720\n","Train Epoch: 47 [51200/60000 (85%)]\tLoss: 28.416164\n","====> Epoch: 47 Average loss: 28.7793\n","Train Epoch: 48 [0/60000 (0%)]\tLoss: 28.572926\n","Train Epoch: 48 [12800/60000 (21%)]\tLoss: 27.849667\n","Train Epoch: 48 [25600/60000 (43%)]\tLoss: 28.376936\n","Train Epoch: 48 [38400/60000 (64%)]\tLoss: 28.620863\n","Train Epoch: 48 [51200/60000 (85%)]\tLoss: 28.506823\n","====> Epoch: 48 Average loss: 28.7629\n","Train Epoch: 49 [0/60000 (0%)]\tLoss: 29.628746\n","Train Epoch: 49 [12800/60000 (21%)]\tLoss: 28.871517\n","Train Epoch: 49 [25600/60000 (43%)]\tLoss: 29.514603\n","Train Epoch: 49 [38400/60000 (64%)]\tLoss: 28.600204\n","Train Epoch: 49 [51200/60000 (85%)]\tLoss: 29.648800\n","====> Epoch: 49 Average loss: 28.7449\n","Train Epoch: 50 [0/60000 (0%)]\tLoss: 28.193184\n","Train Epoch: 50 [12800/60000 (21%)]\tLoss: 28.101582\n","Train Epoch: 50 [25600/60000 (43%)]\tLoss: 27.931593\n","Train Epoch: 50 [38400/60000 (64%)]\tLoss: 29.406269\n","Train Epoch: 50 [51200/60000 (85%)]\tLoss: 28.269871\n","====> Epoch: 50 Average loss: 28.7114\n","Train Epoch: 51 [0/60000 (0%)]\tLoss: 29.710173\n","Train Epoch: 51 [12800/60000 (21%)]\tLoss: 28.895641\n","Train Epoch: 51 [25600/60000 (43%)]\tLoss: 29.583691\n","Train Epoch: 51 [38400/60000 (64%)]\tLoss: 29.016575\n","Train Epoch: 51 [51200/60000 (85%)]\tLoss: 29.145285\n","====> Epoch: 51 Average loss: 28.7065\n","Train Epoch: 52 [0/60000 (0%)]\tLoss: 29.199343\n","Train Epoch: 52 [12800/60000 (21%)]\tLoss: 28.649832\n","Train Epoch: 52 [25600/60000 (43%)]\tLoss: 28.368191\n","Train Epoch: 52 [38400/60000 (64%)]\tLoss: 28.950361\n","Train Epoch: 52 [51200/60000 (85%)]\tLoss: 28.401733\n","====> Epoch: 52 Average loss: 28.7121\n","Train Epoch: 53 [0/60000 (0%)]\tLoss: 28.315443\n","Train Epoch: 53 [12800/60000 (21%)]\tLoss: 28.647459\n","Train Epoch: 53 [25600/60000 (43%)]\tLoss: 28.350597\n","Train Epoch: 53 [38400/60000 (64%)]\tLoss: 29.387966\n","Train Epoch: 53 [51200/60000 (85%)]\tLoss: 29.195396\n","====> Epoch: 53 Average loss: 28.6734\n","Train Epoch: 54 [0/60000 (0%)]\tLoss: 29.104397\n","Train Epoch: 54 [12800/60000 (21%)]\tLoss: 26.553656\n","Train Epoch: 54 [25600/60000 (43%)]\tLoss: 26.842657\n","Train Epoch: 54 [38400/60000 (64%)]\tLoss: 28.759033\n","Train Epoch: 54 [51200/60000 (85%)]\tLoss: 28.904373\n","====> Epoch: 54 Average loss: 28.6572\n","Train Epoch: 55 [0/60000 (0%)]\tLoss: 26.350782\n","Train Epoch: 55 [12800/60000 (21%)]\tLoss: 30.113731\n","Train Epoch: 55 [25600/60000 (43%)]\tLoss: 28.505905\n","Train Epoch: 55 [38400/60000 (64%)]\tLoss: 28.841045\n","Train Epoch: 55 [51200/60000 (85%)]\tLoss: 27.562462\n","====> Epoch: 55 Average loss: 28.6340\n","Train Epoch: 56 [0/60000 (0%)]\tLoss: 29.368557\n","Train Epoch: 56 [12800/60000 (21%)]\tLoss: 28.105431\n","Train Epoch: 56 [25600/60000 (43%)]\tLoss: 29.465036\n","Train Epoch: 56 [38400/60000 (64%)]\tLoss: 29.999485\n","Train Epoch: 56 [51200/60000 (85%)]\tLoss: 29.389959\n","====> Epoch: 56 Average loss: 28.6171\n","Train Epoch: 57 [0/60000 (0%)]\tLoss: 29.129869\n","Train Epoch: 57 [12800/60000 (21%)]\tLoss: 28.307602\n","Train Epoch: 57 [25600/60000 (43%)]\tLoss: 28.018652\n","Train Epoch: 57 [38400/60000 (64%)]\tLoss: 28.980579\n","Train Epoch: 57 [51200/60000 (85%)]\tLoss: 28.405415\n","====> Epoch: 57 Average loss: 28.5954\n","Train Epoch: 58 [0/60000 (0%)]\tLoss: 27.496685\n","Train Epoch: 58 [12800/60000 (21%)]\tLoss: 28.850729\n","Train Epoch: 58 [25600/60000 (43%)]\tLoss: 29.290567\n","Train Epoch: 58 [38400/60000 (64%)]\tLoss: 27.896801\n","Train Epoch: 58 [51200/60000 (85%)]\tLoss: 28.683054\n","====> Epoch: 58 Average loss: 28.5666\n","Train Epoch: 59 [0/60000 (0%)]\tLoss: 29.040279\n","Train Epoch: 59 [12800/60000 (21%)]\tLoss: 27.842428\n","Train Epoch: 59 [25600/60000 (43%)]\tLoss: 27.800728\n","Train Epoch: 59 [38400/60000 (64%)]\tLoss: 30.151293\n","Train Epoch: 59 [51200/60000 (85%)]\tLoss: 26.973000\n","====> Epoch: 59 Average loss: 28.5927\n","Train Epoch: 60 [0/60000 (0%)]\tLoss: 28.657970\n","Train Epoch: 60 [12800/60000 (21%)]\tLoss: 27.550644\n","Train Epoch: 60 [25600/60000 (43%)]\tLoss: 28.129650\n","Train Epoch: 60 [38400/60000 (64%)]\tLoss: 28.832283\n","Train Epoch: 60 [51200/60000 (85%)]\tLoss: 28.480276\n","====> Epoch: 60 Average loss: 28.5521\n","Train Epoch: 61 [0/60000 (0%)]\tLoss: 27.571634\n","Train Epoch: 61 [12800/60000 (21%)]\tLoss: 27.560493\n","Train Epoch: 61 [25600/60000 (43%)]\tLoss: 27.696228\n","Train Epoch: 61 [38400/60000 (64%)]\tLoss: 28.466131\n","Train Epoch: 61 [51200/60000 (85%)]\tLoss: 27.125534\n","====> Epoch: 61 Average loss: 28.5494\n","Train Epoch: 62 [0/60000 (0%)]\tLoss: 30.109463\n","Train Epoch: 62 [12800/60000 (21%)]\tLoss: 27.135365\n","Train Epoch: 62 [25600/60000 (43%)]\tLoss: 27.816055\n","Train Epoch: 62 [38400/60000 (64%)]\tLoss: 28.220186\n","Train Epoch: 62 [51200/60000 (85%)]\tLoss: 28.200626\n","====> Epoch: 62 Average loss: 28.5182\n","Train Epoch: 63 [0/60000 (0%)]\tLoss: 28.432617\n","Train Epoch: 63 [12800/60000 (21%)]\tLoss: 27.171879\n","Train Epoch: 63 [25600/60000 (43%)]\tLoss: 28.812885\n","Train Epoch: 63 [38400/60000 (64%)]\tLoss: 29.080109\n","Train Epoch: 63 [51200/60000 (85%)]\tLoss: 30.318230\n","====> Epoch: 63 Average loss: 28.4956\n","Train Epoch: 64 [0/60000 (0%)]\tLoss: 28.489174\n","Train Epoch: 64 [12800/60000 (21%)]\tLoss: 29.301821\n","Train Epoch: 64 [25600/60000 (43%)]\tLoss: 28.478935\n","Train Epoch: 64 [38400/60000 (64%)]\tLoss: 28.475962\n","Train Epoch: 64 [51200/60000 (85%)]\tLoss: 29.595627\n","====> Epoch: 64 Average loss: 28.4659\n","Train Epoch: 65 [0/60000 (0%)]\tLoss: 29.545691\n","Train Epoch: 65 [12800/60000 (21%)]\tLoss: 28.004623\n","Train Epoch: 65 [25600/60000 (43%)]\tLoss: 27.851486\n","Train Epoch: 65 [38400/60000 (64%)]\tLoss: 28.679453\n","Train Epoch: 65 [51200/60000 (85%)]\tLoss: 28.515099\n","====> Epoch: 65 Average loss: 28.4770\n","Train Epoch: 66 [0/60000 (0%)]\tLoss: 29.209038\n","Train Epoch: 66 [12800/60000 (21%)]\tLoss: 28.248577\n","Train Epoch: 66 [25600/60000 (43%)]\tLoss: 27.441257\n","Train Epoch: 66 [38400/60000 (64%)]\tLoss: 27.771910\n","Train Epoch: 66 [51200/60000 (85%)]\tLoss: 29.556868\n","====> Epoch: 66 Average loss: 28.4763\n","Train Epoch: 67 [0/60000 (0%)]\tLoss: 29.115997\n","Train Epoch: 67 [12800/60000 (21%)]\tLoss: 28.156624\n","Train Epoch: 67 [25600/60000 (43%)]\tLoss: 28.498190\n","Train Epoch: 67 [38400/60000 (64%)]\tLoss: 28.918692\n","Train Epoch: 67 [51200/60000 (85%)]\tLoss: 27.893799\n","====> Epoch: 67 Average loss: 28.4531\n","Train Epoch: 68 [0/60000 (0%)]\tLoss: 28.075695\n","Train Epoch: 68 [12800/60000 (21%)]\tLoss: 28.552080\n","Train Epoch: 68 [25600/60000 (43%)]\tLoss: 27.768383\n","Train Epoch: 68 [38400/60000 (64%)]\tLoss: 27.912237\n","Train Epoch: 68 [51200/60000 (85%)]\tLoss: 29.075356\n","====> Epoch: 68 Average loss: 28.4387\n","Train Epoch: 69 [0/60000 (0%)]\tLoss: 29.729492\n","Train Epoch: 69 [12800/60000 (21%)]\tLoss: 28.751093\n","Train Epoch: 69 [25600/60000 (43%)]\tLoss: 29.493813\n","Train Epoch: 69 [38400/60000 (64%)]\tLoss: 28.651052\n","Train Epoch: 69 [51200/60000 (85%)]\tLoss: 28.575958\n","====> Epoch: 69 Average loss: 28.4163\n","Train Epoch: 70 [0/60000 (0%)]\tLoss: 28.040705\n","Train Epoch: 70 [12800/60000 (21%)]\tLoss: 27.941366\n","Train Epoch: 70 [25600/60000 (43%)]\tLoss: 28.783138\n","Train Epoch: 70 [38400/60000 (64%)]\tLoss: 28.006195\n","Train Epoch: 70 [51200/60000 (85%)]\tLoss: 28.332455\n","====> Epoch: 70 Average loss: 28.4251\n","Train Epoch: 71 [0/60000 (0%)]\tLoss: 28.817966\n","Train Epoch: 71 [12800/60000 (21%)]\tLoss: 28.160517\n","Train Epoch: 71 [25600/60000 (43%)]\tLoss: 27.482975\n","Train Epoch: 71 [38400/60000 (64%)]\tLoss: 28.392345\n","Train Epoch: 71 [51200/60000 (85%)]\tLoss: 29.253056\n","====> Epoch: 71 Average loss: 28.4006\n","Train Epoch: 72 [0/60000 (0%)]\tLoss: 28.112411\n","Train Epoch: 72 [12800/60000 (21%)]\tLoss: 29.035429\n","Train Epoch: 72 [25600/60000 (43%)]\tLoss: 27.335148\n","Train Epoch: 72 [38400/60000 (64%)]\tLoss: 28.656538\n","Train Epoch: 72 [51200/60000 (85%)]\tLoss: 28.828176\n","====> Epoch: 72 Average loss: 28.3971\n","Train Epoch: 73 [0/60000 (0%)]\tLoss: 27.546986\n","Train Epoch: 73 [12800/60000 (21%)]\tLoss: 28.144688\n","Train Epoch: 73 [25600/60000 (43%)]\tLoss: 28.353193\n","Train Epoch: 73 [38400/60000 (64%)]\tLoss: 28.515383\n","Train Epoch: 73 [51200/60000 (85%)]\tLoss: 29.760624\n","====> Epoch: 73 Average loss: 28.3775\n","Train Epoch: 74 [0/60000 (0%)]\tLoss: 27.444118\n","Train Epoch: 74 [12800/60000 (21%)]\tLoss: 28.236748\n","Train Epoch: 74 [25600/60000 (43%)]\tLoss: 28.271450\n","Train Epoch: 74 [38400/60000 (64%)]\tLoss: 28.210264\n","Train Epoch: 74 [51200/60000 (85%)]\tLoss: 29.174770\n","====> Epoch: 74 Average loss: 28.3835\n","Train Epoch: 75 [0/60000 (0%)]\tLoss: 26.182407\n","Train Epoch: 75 [12800/60000 (21%)]\tLoss: 27.830315\n","Train Epoch: 75 [25600/60000 (43%)]\tLoss: 27.843735\n","Train Epoch: 75 [38400/60000 (64%)]\tLoss: 28.300385\n","Train Epoch: 75 [51200/60000 (85%)]\tLoss: 29.098995\n","====> Epoch: 75 Average loss: 28.3464\n","Train Epoch: 76 [0/60000 (0%)]\tLoss: 27.497347\n","Train Epoch: 76 [12800/60000 (21%)]\tLoss: 30.479469\n","Train Epoch: 76 [25600/60000 (43%)]\tLoss: 30.659098\n","Train Epoch: 76 [38400/60000 (64%)]\tLoss: 26.906513\n","Train Epoch: 76 [51200/60000 (85%)]\tLoss: 27.231108\n","====> Epoch: 76 Average loss: 28.3488\n","Train Epoch: 77 [0/60000 (0%)]\tLoss: 29.328089\n","Train Epoch: 77 [12800/60000 (21%)]\tLoss: 28.362568\n","Train Epoch: 77 [25600/60000 (43%)]\tLoss: 30.009245\n","Train Epoch: 77 [38400/60000 (64%)]\tLoss: 29.516903\n","Train Epoch: 77 [51200/60000 (85%)]\tLoss: 29.779669\n","====> Epoch: 77 Average loss: 28.3474\n","Train Epoch: 78 [0/60000 (0%)]\tLoss: 29.402050\n","Train Epoch: 78 [12800/60000 (21%)]\tLoss: 29.816296\n","Train Epoch: 78 [25600/60000 (43%)]\tLoss: 29.264259\n","Train Epoch: 78 [38400/60000 (64%)]\tLoss: 28.645874\n","Train Epoch: 78 [51200/60000 (85%)]\tLoss: 26.750446\n","====> Epoch: 78 Average loss: 28.3193\n","Train Epoch: 79 [0/60000 (0%)]\tLoss: 27.042763\n","Train Epoch: 79 [12800/60000 (21%)]\tLoss: 28.318409\n","Train Epoch: 79 [25600/60000 (43%)]\tLoss: 28.064890\n","Train Epoch: 79 [38400/60000 (64%)]\tLoss: 26.968563\n","Train Epoch: 79 [51200/60000 (85%)]\tLoss: 27.907818\n","====> Epoch: 79 Average loss: 28.3129\n","Train Epoch: 80 [0/60000 (0%)]\tLoss: 27.791386\n","Train Epoch: 80 [12800/60000 (21%)]\tLoss: 28.518272\n","Train Epoch: 80 [25600/60000 (43%)]\tLoss: 27.271124\n","Train Epoch: 80 [38400/60000 (64%)]\tLoss: 28.639780\n","Train Epoch: 80 [51200/60000 (85%)]\tLoss: 27.474678\n","====> Epoch: 80 Average loss: 28.3040\n","Train Epoch: 81 [0/60000 (0%)]\tLoss: 29.324747\n","Train Epoch: 81 [12800/60000 (21%)]\tLoss: 28.727657\n","Train Epoch: 81 [25600/60000 (43%)]\tLoss: 29.233456\n","Train Epoch: 81 [38400/60000 (64%)]\tLoss: 28.199833\n","Train Epoch: 81 [51200/60000 (85%)]\tLoss: 28.135792\n","====> Epoch: 81 Average loss: 28.3064\n","Train Epoch: 82 [0/60000 (0%)]\tLoss: 27.640953\n","Train Epoch: 82 [12800/60000 (21%)]\tLoss: 28.867266\n","Train Epoch: 82 [25600/60000 (43%)]\tLoss: 28.726257\n","Train Epoch: 82 [38400/60000 (64%)]\tLoss: 28.023756\n","Train Epoch: 82 [51200/60000 (85%)]\tLoss: 28.037239\n","====> Epoch: 82 Average loss: 28.3157\n","Train Epoch: 83 [0/60000 (0%)]\tLoss: 29.532753\n","Train Epoch: 83 [12800/60000 (21%)]\tLoss: 27.474199\n","Train Epoch: 83 [25600/60000 (43%)]\tLoss: 27.271473\n","Train Epoch: 83 [38400/60000 (64%)]\tLoss: 27.873428\n","Train Epoch: 83 [51200/60000 (85%)]\tLoss: 29.293295\n","====> Epoch: 83 Average loss: 28.2938\n","Train Epoch: 84 [0/60000 (0%)]\tLoss: 27.396664\n","Train Epoch: 84 [12800/60000 (21%)]\tLoss: 28.305662\n","Train Epoch: 84 [25600/60000 (43%)]\tLoss: 27.835703\n","Train Epoch: 84 [38400/60000 (64%)]\tLoss: 28.268219\n","Train Epoch: 84 [51200/60000 (85%)]\tLoss: 29.028370\n","====> Epoch: 84 Average loss: 28.2724\n","Train Epoch: 85 [0/60000 (0%)]\tLoss: 28.702179\n","Train Epoch: 85 [12800/60000 (21%)]\tLoss: 28.965387\n","Train Epoch: 85 [25600/60000 (43%)]\tLoss: 27.738327\n","Train Epoch: 85 [38400/60000 (64%)]\tLoss: 28.208410\n","Train Epoch: 85 [51200/60000 (85%)]\tLoss: 27.432755\n","====> Epoch: 85 Average loss: 28.2583\n","Train Epoch: 86 [0/60000 (0%)]\tLoss: 28.780573\n","Train Epoch: 86 [12800/60000 (21%)]\tLoss: 27.432205\n","Train Epoch: 86 [25600/60000 (43%)]\tLoss: 26.942474\n","Train Epoch: 86 [38400/60000 (64%)]\tLoss: 27.776472\n","Train Epoch: 86 [51200/60000 (85%)]\tLoss: 28.284044\n","====> Epoch: 86 Average loss: 28.2665\n","Train Epoch: 87 [0/60000 (0%)]\tLoss: 27.885746\n","Train Epoch: 87 [12800/60000 (21%)]\tLoss: 27.777567\n","Train Epoch: 87 [25600/60000 (43%)]\tLoss: 27.692680\n","Train Epoch: 87 [38400/60000 (64%)]\tLoss: 28.677929\n","Train Epoch: 87 [51200/60000 (85%)]\tLoss: 28.983223\n","====> Epoch: 87 Average loss: 28.2477\n","Train Epoch: 88 [0/60000 (0%)]\tLoss: 28.514202\n","Train Epoch: 88 [12800/60000 (21%)]\tLoss: 27.500504\n","Train Epoch: 88 [25600/60000 (43%)]\tLoss: 28.061613\n","Train Epoch: 88 [38400/60000 (64%)]\tLoss: 27.152065\n","Train Epoch: 88 [51200/60000 (85%)]\tLoss: 27.975750\n","====> Epoch: 88 Average loss: 28.2638\n","Train Epoch: 89 [0/60000 (0%)]\tLoss: 28.483109\n","Train Epoch: 89 [12800/60000 (21%)]\tLoss: 28.744310\n","Train Epoch: 89 [25600/60000 (43%)]\tLoss: 27.136452\n","Train Epoch: 89 [38400/60000 (64%)]\tLoss: 28.514736\n","Train Epoch: 89 [51200/60000 (85%)]\tLoss: 28.810520\n","====> Epoch: 89 Average loss: 28.2289\n","Train Epoch: 90 [0/60000 (0%)]\tLoss: 27.983288\n","Train Epoch: 90 [12800/60000 (21%)]\tLoss: 27.386101\n","Train Epoch: 90 [25600/60000 (43%)]\tLoss: 28.352024\n","Train Epoch: 90 [38400/60000 (64%)]\tLoss: 26.708164\n","Train Epoch: 90 [51200/60000 (85%)]\tLoss: 27.742126\n","====> Epoch: 90 Average loss: 28.2295\n","Train Epoch: 91 [0/60000 (0%)]\tLoss: 27.248272\n","Train Epoch: 91 [12800/60000 (21%)]\tLoss: 29.184702\n","Train Epoch: 91 [25600/60000 (43%)]\tLoss: 27.378788\n","Train Epoch: 91 [38400/60000 (64%)]\tLoss: 29.437193\n","Train Epoch: 91 [51200/60000 (85%)]\tLoss: 27.273746\n","====> Epoch: 91 Average loss: 28.2500\n","Train Epoch: 92 [0/60000 (0%)]\tLoss: 28.187714\n","Train Epoch: 92 [12800/60000 (21%)]\tLoss: 28.301731\n","Train Epoch: 92 [25600/60000 (43%)]\tLoss: 28.451492\n","Train Epoch: 92 [38400/60000 (64%)]\tLoss: 27.563234\n","Train Epoch: 92 [51200/60000 (85%)]\tLoss: 27.978546\n","====> Epoch: 92 Average loss: 28.2062\n","Train Epoch: 93 [0/60000 (0%)]\tLoss: 28.795551\n","Train Epoch: 93 [12800/60000 (21%)]\tLoss: 29.440331\n","Train Epoch: 93 [25600/60000 (43%)]\tLoss: 28.008291\n","Train Epoch: 93 [38400/60000 (64%)]\tLoss: 27.995909\n","Train Epoch: 93 [51200/60000 (85%)]\tLoss: 27.551445\n","====> Epoch: 93 Average loss: 28.2087\n","Train Epoch: 94 [0/60000 (0%)]\tLoss: 26.988396\n","Train Epoch: 94 [12800/60000 (21%)]\tLoss: 28.492996\n","Train Epoch: 94 [25600/60000 (43%)]\tLoss: 27.184383\n","Train Epoch: 94 [38400/60000 (64%)]\tLoss: 28.212875\n","Train Epoch: 94 [51200/60000 (85%)]\tLoss: 28.892012\n","====> Epoch: 94 Average loss: 28.2064\n","Train Epoch: 95 [0/60000 (0%)]\tLoss: 27.930775\n","Train Epoch: 95 [12800/60000 (21%)]\tLoss: 26.815670\n","Train Epoch: 95 [25600/60000 (43%)]\tLoss: 29.793865\n","Train Epoch: 95 [38400/60000 (64%)]\tLoss: 28.267349\n","Train Epoch: 95 [51200/60000 (85%)]\tLoss: 27.860188\n","====> Epoch: 95 Average loss: 28.2004\n","Train Epoch: 96 [0/60000 (0%)]\tLoss: 28.820621\n","Train Epoch: 96 [12800/60000 (21%)]\tLoss: 27.389772\n","Train Epoch: 96 [25600/60000 (43%)]\tLoss: 27.319649\n","Train Epoch: 96 [38400/60000 (64%)]\tLoss: 28.458767\n","Train Epoch: 96 [51200/60000 (85%)]\tLoss: 28.006496\n","====> Epoch: 96 Average loss: 28.1980\n","Train Epoch: 97 [0/60000 (0%)]\tLoss: 28.618162\n","Train Epoch: 97 [12800/60000 (21%)]\tLoss: 28.995243\n","Train Epoch: 97 [25600/60000 (43%)]\tLoss: 28.852652\n","Train Epoch: 97 [38400/60000 (64%)]\tLoss: 28.830608\n","Train Epoch: 97 [51200/60000 (85%)]\tLoss: 29.681858\n","====> Epoch: 97 Average loss: 28.1707\n","Train Epoch: 98 [0/60000 (0%)]\tLoss: 28.547653\n","Train Epoch: 98 [12800/60000 (21%)]\tLoss: 28.696363\n","Train Epoch: 98 [25600/60000 (43%)]\tLoss: 27.857784\n","Train Epoch: 98 [38400/60000 (64%)]\tLoss: 28.074005\n","Train Epoch: 98 [51200/60000 (85%)]\tLoss: 28.585857\n","====> Epoch: 98 Average loss: 28.1815\n","Train Epoch: 99 [0/60000 (0%)]\tLoss: 28.612989\n","Train Epoch: 99 [12800/60000 (21%)]\tLoss: 28.504051\n","Train Epoch: 99 [25600/60000 (43%)]\tLoss: 28.616253\n","Train Epoch: 99 [38400/60000 (64%)]\tLoss: 28.193943\n","Train Epoch: 99 [51200/60000 (85%)]\tLoss: 28.936710\n","====> Epoch: 99 Average loss: 28.1916\n"]}],"source":["# Alleniamo il nostro VAE\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0\n","    for batch_idx, data in enumerate(dataloader):\n","        img, _ = data\n","        img = img.view(img.size(0), -1)\n","        if torch.cuda.is_available():\n","            img = img.cuda()\n","        optimizer.zero_grad()\n","        recon_batch, mu, logvar = model(img)\n","        loss = loss_function(recon_batch, img, mu, logvar)\n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","        if batch_idx % 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch,\n","                batch_idx * len(img),\n","                len(dataloader.dataset), 100. * batch_idx / len(dataloader),\n","                loss.item() / len(img)))\n","\n","    print('====> Epoch: {} Average loss: {:.4f}'.format(\n","        epoch, train_loss / len(dataloader.dataset)))\n","    if epoch % 10 == 0:\n","        save = to_img(recon_batch.cpu().data)\n","        save_image(save, './vae_img/image_{}.png'.format(epoch))\n","\n","torch.save(model.state_dict(), './vae.pth')"]},{"cell_type":"markdown","source":["Il VAE che abbiamo utilizzato in questo notebook è molto semplice. E' costruito con layer fully connected, mentre per lavorare con le immagini, come abbiamo visto, è molto più pratico utilizzare reti convoluzionali. Per esercizi più complessi, ti consiglio quindi di costruire VAE con reti convoluzionali, come ad esempio quello qui sotto."],"metadata":{"id":"ojovgZYdXwGg"}},{"cell_type":"code","source":["class ConvVAE(BaseVAE):\n","\n","    def __init__(self,\n","                 in_channels: int,\n","                 latent_dim: int,\n","                 hidden_dims: List = None,\n","                 **kwargs) -> None:\n","        super(ConvVAE, self).__init__()\n","\n","        self.latent_dim = latent_dim\n","        modules = []\n","        if hidden_dims is None:\n","            hidden_dims = [32, 64, 128, 256, 512]\n","\n","        # Costruiamo l'encoder\n","        for h_dim in hidden_dims:\n","            modules.append(\n","                nn.Sequential(\n","                    nn.Conv2d(in_channels, out_channels=h_dim,\n","                              kernel_size= 3, stride= 2, padding  = 1),\n","                    nn.BatchNorm2d(h_dim),\n","                    nn.LeakyReLU())\n","            )\n","            in_channels = h_dim\n","\n","        self.encoder = nn.Sequential(*modules)\n","        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n","        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n","\n","        # Costruiamo il decoder\n","        modules = []\n","        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n","        hidden_dims.reverse()\n","\n","        for i in range(len(hidden_dims) - 1):\n","            modules.append(\n","                nn.Sequential(\n","                    nn.ConvTranspose2d(hidden_dims[i],\n","                                       hidden_dims[i + 1],\n","                                       kernel_size=3,\n","                                       stride = 2,\n","                                       padding=1,\n","                                       output_padding=1),\n","                    nn.BatchNorm2d(hidden_dims[i + 1]),\n","                    nn.LeakyReLU())\n","            )\n","        self.decoder = nn.Sequential(*modules)\n","        self.final_layer = nn.Sequential(\n","                            nn.ConvTranspose2d(hidden_dims[-1],\n","                                               hidden_dims[-1],\n","                                               kernel_size=3,\n","                                               stride=2,\n","                                               padding=1,\n","                                               output_padding=1),\n","                            nn.BatchNorm2d(hidden_dims[-1]),\n","                            nn.LeakyReLU(),\n","                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n","                                      kernel_size= 3, padding= 1),\n","                            nn.Tanh())\n","\n","    def encode(self, input):\n","        result = self.encoder(input)\n","        result = torch.flatten(result, start_dim=1)\n","\n","        mu = self.fc_mu(result)\n","        log_var = self.fc_var(result)\n","\n","        return [mu, log_var]\n","\n","    def decode(self, z):\n","        result = self.decoder_input(z)\n","        result = result.view(-1, 512, 2, 2)\n","        result = self.decoder(result)\n","        result = self.final_layer(result)\n","        return result\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return eps * std + mu\n","\n","    def forward(self, input, **kwargs):\n","        mu, log_var = self.encode(input)\n","        z = self.reparameterize(mu, log_var)\n","        return  [self.decode(z), input, mu, log_var]"],"metadata":{"id":"ErzgBW22jWHz"},"execution_count":null,"outputs":[]}]}