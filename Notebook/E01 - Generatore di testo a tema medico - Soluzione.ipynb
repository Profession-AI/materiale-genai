{"cells":[{"cell_type":"markdown","source":["### Prendi sempre il dataset fetch_20newsgroups. Questa volta però filtra per le news a tema medico (utilizza la categoria sci.med)\n","- Crea un corpus prendendo tutte le frasi delle prime 100 news\n","- Fai preprocessing come nel notebook precedente per generare dei 4-Grammi\n","- Non considerare come vocabolario tutte le parole presenti nelle news considerate, ma prova a tenere solamente le 3000 più presenti\n","- Calcola la matrice di probabilità per solamente per gli n-grammi presenti nel corpus\n","- Crea un Language Model come nel notebook precedente e genera del testo\n","- Prova a generare del testo sia non facendo random sampling sia facendolo\n","- Prova a variare la soglia di threshold (con range tra 0.0000001 e 0.0003) sopra la quale tenere i token da generare per vedere (anche con un piccolo esempio) come cambiano i testi generati dal language model\n","\n"],"metadata":{"id":"Y-xcqSLlcJYu"}},{"cell_type":"markdown","source":["<font color='red'>Va BENISSIMO fare copia-incolla dai notebook precedenti: <br>\n","&nbsp;&nbsp;&nbsp;&nbsp; → non devi imparare cose a memoria, devi capire come funzionano e sapere dove copiare, cosa modifiare e come! <br>\n"," Riesci a superare tutti gli errori ed a generare del testo a tema medico?</font>"],"metadata":{"id":"NyBlrL7qc_ym"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5401,"status":"ok","timestamp":1699478073459,"user":{"displayName":"Luca Negri","userId":"08467063413129502685"},"user_tz":-60},"id":"fGHwwchkIdcA","outputId":"54114bac-b9d3-4372-c207-2571d5c9d5a6"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}],"source":["from sklearn.datasets import fetch_20newsgroups         # Dataset che contiene testo di articoli di giornale appartenenti a 20 categorie differenti\n","import nltk                                             # NLP toolkit\n","import re                                               # Libreria per operazioni con le espressioni regolari\n","from collections import defaultdict, Counter\n","import numpy as np\n","import pandas as pd\n","import random\n","import itertools\n","\n","nltk.download('punkt')                                  # Con questo comando si scarica il tokenizzatore 'Punkt'"]},{"cell_type":"markdown","metadata":{"id":"-pPB2noLN26s"},"source":["## Scarichiamo il nostro dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kx_TWkYlL1WA"},"outputs":[],"source":["train_news_texts, category = fetch_20newsgroups(subset=\"train\", categories=[\"sci.med\"], return_X_y=True, remove=['headers', 'footers', 'quotes'])"]},{"cell_type":"markdown","source":["## Definiamo la funzione creata in precedenza per processare il dataset e creare gli N-Grammi"],"metadata":{"id":"sUOUZdibZZJx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWAMly0ZUFAQ"},"outputs":[],"source":["def preprocessing(text):\n","    \"\"\"\n","    Funzione che preprocessa il testo per creare una frase tokenizzata.\n","\n","    Args:\n","        text: stringa contenente il testo da preprocessare e tokenizzare\n","\n","    Returns:\n","        testo preprocessato e tokenizzato\n","    \"\"\"\n","    text = text.lower()\n","    text = text.replace('\\n', ' ')\n","    text = re.sub(r'[^a-zA-Z0-9.?! ]+', '', text)\n","    text = re.sub(' +', ' ', text)\n","    text = text.strip()\n","    text_tokenized = nltk.word_tokenize(text)\n","    return text_tokenized\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s608UvXxRCOz"},"outputs":[],"source":["def sentence_to_ngram(tokenized_sentence, n=3):\n","    \"\"\"\n","    Funzione che restituisce tutti i n-grammi contenuti all'interno della frase tokenizzata.\n","\n","    Args:\n","        tokenized_sentence: lista di parole/tokens che compongono la frase\n","        n: n-grammi da considerare\n","\n","    Returns:\n","        lista di tutti i n-grammi presenti all'interno della frase tokenizzata\n","    \"\"\"\n","\n","    ngram_list = []\n","    tokenized_sentence = ['<s>'] * (n - 1) + tokenized_sentence + ['</s>']\n","    for i in range(len(tokenized_sentence) - n + 1):\n","        # the sliding window starts at position i and contains 3 words\n","        ngram = tokenized_sentence[i : i + n]\n","        ngram_list.append(ngram)\n","    return ngram_list\n","\n"]},{"cell_type":"markdown","source":["# Prendi il sample di testi che useremo come base per costruire il Language Model\n","\n"],"metadata":{"id":"U9uTma2AaA8v"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1699478086642,"user":{"displayName":"Luca Negri","userId":"08467063413129502685"},"user_tz":-60},"id":"vZonBlLDT274","colab":{"base_uri":"https://localhost:8080/"},"outputId":"21c77fdd-04cd-4b3b-e823-74d077c604b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["[reply to keith@actrix.gen.nz (Keith Stewart)]\n"," \n"," \n","It would help if you (and anyone else asking for medical information on\n","some subject) could ask specific questions, as no one is likely to type\n","in a textbook chapter covering all aspects of the subject.\n","--------------------------------------------------------------------------------\n","If you are\n","looking for a comprehensive review, ask your local hospital librarian.\n","--------------------------------------------------------------------------------\n","Most are happy to help with a request of this sort.\n","--------------------------------------------------------------------------------\n","Briefly, this is a condition in which patients who have significant\n","residual weakness from childhood polio notice progression of the\n","weakness as they get older.\n","--------------------------------------------------------------------------------\n","One theory is that the remaining motor\n","neurons have to work harder and so die sooner.\n","--------------------------------------------------------------------------------\n","I had allergy shots for about four years starting as a sophomore in high\n","school.\n","--------------------------------------------------------------------------------\n","Before that, I used to get bloody noses, nighttime asthma attacks,\n","and eyes so itchy I couldn't get to sleep.\n","--------------------------------------------------------------------------------\n","After about 6 months on the\n","shots, most of those symptoms were gone, and they haven't come back.\n","--------------------------------------------------------------------------------\n","I\n","stopped getting the shots (due more to laziness than planning) in college.\n","--------------------------------------------------------------------------------\n","My allergies got a little worse after that, but are still nowhere near as\n","bad as they used to be.\n","--------------------------------------------------------------------------------\n"]}],"source":["train_news_texts_sample = [sentence for x in train_news_texts[:100] for sentence in nltk.sent_tokenize(x.strip())]\n","for text in train_news_texts_sample[:10]:\n","  print(text)\n","  print(\"-\"*80)\n"]},{"cell_type":"markdown","source":["# Definisci il vocabolario tenendo solamente le prime 3000 parole più comuni"],"metadata":{"id":"31CUozRqBQ_5"}},{"cell_type":"code","source":["train_news_texts_sample_processed = [preprocessing(text) for text in train_news_texts_sample]"],"metadata":{"id":"Noq4rZDMN5VS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["M = 3000\n","corpus = [x for y in train_news_texts_sample_processed for x in y]\n","word_count = Counter(corpus)\n","vocabulary = Counter(word_count).most_common(M)\n","vocabulary = [word for word, frequency in vocabulary]"],"metadata":{"id":"pujv02EMBCV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Siccome abbiamo delle parole non presenti nel nostro vocabolario, aggiungiamoci anche il token UNK e i token di inizio/fine frase\n","vocabulary.append('UNK')\n","vocabulary.append('<s>')\n","vocabulary.append('</s>')"],"metadata":{"id":"TYMcN4zAF3KH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_news_texts_sample_processed_unk = []\n","for sentence in train_news_texts_sample_processed:\n","  new_sentence = []\n","  for word in sentence:\n","    if word not in vocabulary:\n","      new_sentence.append('UNK')\n","    else:\n","      new_sentence.append(word)\n","  train_news_texts_sample_processed_unk.append(new_sentence)"],"metadata":{"id":"FChgE3XxF8Sv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = 4\n","quadrigrams_list = [sentence_to_ngram(tokenized_sentence, n) for tokenized_sentence in train_news_texts_sample_processed_unk]\n","quadrigrams_list = [tuple(x) for y in quadrigrams_list for x in y]\n","count_quadrigrams = Counter(quadrigrams_list)\n","count_quadrigrams.most_common(20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5yHbH9fZ_wi","executionInfo":{"status":"ok","timestamp":1699478087296,"user_tz":-60,"elapsed":5,"user":{"displayName":"Luca Negri","userId":"08467063413129502685"}},"outputId":"00211a5f-a713-476d-f18f-0b0e502bc58c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('<s>', '<s>', '<s>', 'UNK'), 129),\n"," (('<s>', '<s>', '<s>', 'the'), 121),\n"," (('<s>', '<s>', '<s>', 'i'), 86),\n"," (('UNK', 'UNK', '.', '</s>'), 50),\n"," (('<s>', '<s>', '<s>', 'this'), 39),\n"," (('<s>', '<s>', 'UNK', 'UNK'), 36),\n"," (('<s>', '<s>', '<s>', 'if'), 35),\n"," (('<s>', '<s>', '<s>', 'it'), 34),\n"," (('<s>', '<s>', '<s>', 'in'), 31),\n"," (('<s>', '<s>', '<s>', 'a'), 26),\n"," (('volume', '6', 'number', '10'), 25),\n"," (('6', 'number', '10', 'april'), 25),\n"," (('number', '10', 'april', '20'), 25),\n"," (('10', 'april', '20', '1993'), 25),\n"," (('hicnet', 'medical', 'newsletter', 'page'), 25),\n"," (('UNK', 'UNK', 'UNK', 'UNK'), 22),\n"," (('<s>', '<s>', '<s>', 'what'), 19),\n"," (('<s>', '<s>', '<s>', 'but'), 16),\n"," (('<s>', '<s>', 'if', 'you'), 15),\n"," (('<s>', '<s>', '<s>', 'and'), 15)]"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# Crea la matrice dei conteggi"],"metadata":{"id":"eynkqJFzIRGG"}},{"cell_type":"code","source":["def single_pass_ngram_count_matrix(count_ngrams):\n","    \"\"\"\n","    Crea la matrice dei conteggi di tri-grammi utilizzando il corpus passato in input.\n","\n","    Args:\n","        count_ngrams: Conteggio dei n-grammi presenti nel corpus\n","\n","    Returns:\n","        n_minus_one_grams: lista di tutti i bigrammi, utilizzato come indice di riga della matrice\n","        vocabulary: lista di tutte le parole presenti nel corpus, utilizzato come indice di colonna\n","        count_matrix: pandas dataframe con i bigrammi prefixes come righe,\n","                      le parole del vocabolario come colonne\n","                      e il conteggio delle combinazioni bigramma/parola come valore\n","    \"\"\"\n","    n_minus_one_grams = []\n","    vocabulary = []\n","    count_matrix_dict = defaultdict(dict)\n","\n","    # go through the corpus once with a sliding window\n","    for ngram, count in count_ngrams.items():\n","\n","        n_minus_one_gram = ngram[0 : -1]\n","        if not n_minus_one_gram in n_minus_one_grams:\n","            n_minus_one_grams.append(n_minus_one_gram)\n","\n","        last_word = ngram[-1]\n","        if not last_word in vocabulary:\n","            vocabulary.append(last_word)\n","\n","        if (n_minus_one_gram,last_word) not in count_matrix_dict:\n","            count_matrix_dict[n_minus_one_gram,last_word] = count\n","\n","    # convert the count_matrix to np.array to fill in the blanks\n","    count_matrix = np.zeros((len(n_minus_one_grams), len(vocabulary)))\n","    for ngram_key, ngram_count in count_matrix_dict.items():\n","        count_matrix[n_minus_one_grams.index(ngram_key[0]), \\\n","                     vocabulary.index(ngram_key[1])]\\\n","        = ngram_count\n","\n","    # np.array to pandas dataframe conversion\n","    count_matrix = pd.DataFrame(count_matrix, index=n_minus_one_grams, columns=vocabulary)\n","    return n_minus_one_grams, vocabulary, count_matrix\n"],"metadata":{"id":"aiXMKWhoal2-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_minus_one_grams, vocabulary, count_matrix = single_pass_ngram_count_matrix(count_quadrigrams)\n","\n","print(count_matrix.iloc[:5, :5])"],"metadata":{"id":"CfA0EMLCck2r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699478104141,"user_tz":-60,"elapsed":16848,"user":{"displayName":"Luca Negri","userId":"08467063413129502685"}},"outputId":"54cdd418-0d9b-49e6-a0bb-ed3aef75c20d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                 reply   to  keithactrix.gen.nz  keith  \\\n","(<s>, <s>, <s>)                    1.0  7.0                 0.0    0.0   \n","(<s>, <s>, reply)                  0.0  1.0                 0.0    0.0   \n","(<s>, reply, to)                   0.0  0.0                 1.0    0.0   \n","(reply, to, keithactrix.gen.nz)    0.0  0.0                 0.0    1.0   \n","(to, keithactrix.gen.nz, keith)    0.0  0.0                 0.0    0.0   \n","\n","                                 stewart  \n","(<s>, <s>, <s>)                      0.0  \n","(<s>, <s>, reply)                    0.0  \n","(<s>, reply, to)                     0.0  \n","(reply, to, keithactrix.gen.nz)      0.0  \n","(to, keithactrix.gen.nz, keith)      1.0  \n"]}]},{"cell_type":"code","source":["# create the probability matrix from the count matrix\n","row_sums = count_matrix.sum(axis=1)\n","# delete each row by its sum\n","prob_matrix = count_matrix.div(row_sums, axis=0)\n","\n","print(prob_matrix.iloc[:5, :5])\n"],"metadata":{"id":"lHPwW-7Bgxzd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699478104524,"user_tz":-60,"elapsed":398,"user":{"displayName":"Luca Negri","userId":"08467063413129502685"}},"outputId":"27a124bc-ab88-478e-c01a-b3ef85cade74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                    reply        to  keithactrix.gen.nz  \\\n","(<s>, <s>, <s>)                  0.000729  0.005102                 0.0   \n","(<s>, <s>, reply)                0.000000  1.000000                 0.0   \n","(<s>, reply, to)                 0.000000  0.000000                 1.0   \n","(reply, to, keithactrix.gen.nz)  0.000000  0.000000                 0.0   \n","(to, keithactrix.gen.nz, keith)  0.000000  0.000000                 0.0   \n","\n","                                 keith  stewart  \n","(<s>, <s>, <s>)                    0.0      0.0  \n","(<s>, <s>, reply)                  0.0      0.0  \n","(<s>, reply, to)                   0.0      0.0  \n","(reply, to, keithactrix.gen.nz)    1.0      0.0  \n","(to, keithactrix.gen.nz, keith)    0.0      1.0  \n"]}]},{"cell_type":"code","source":["print(prob_matrix.max().max())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_DMG6HQuMgS5","executionInfo":{"status":"ok","timestamp":1699478104798,"user_tz":-60,"elapsed":275,"user":{"displayName":"Luca Negri","userId":"08467063413129502685"}},"outputId":"5449ee58-8b0d-400a-dcc3-e3805d9969fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.0\n"]}]},{"cell_type":"markdown","source":["## Generiamo del testo utilizzando il nostro language model"],"metadata":{"id":"4SpXRpwVSV3c"}},{"cell_type":"code","source":["def generate_text(n: int, prob_matrix: pd.DataFrame,\n","                  token_count: int, threshold_prob = 0.005, random_sampling: bool = False):\n","    \"\"\"\n","    Funzione per generare del testo partendo dalla matrice di probabilità.\n","\n","    Args:\n","        n: modello n-gramma da utilizzare\n","        prob_matrix: matrice di probabilità\n","        token_count: numero di token da generare\n","        threshold_prob: soglia di probabilità sopra la quale considerare i token\n","        random_sampling: booleano che dice se effettuare un sampling tra i token a probabilità non nulla oppure prendere sempre quello a probabilità massima\n","\n","    Returns:\n","        bigrams: lista di tutti i bigrammi, utilizzato come indice di riga della matrice\n","        vocabulary: lista di tutte le parole presenti nel corpus, utilizzato come indice di colonna\n","        count_matrix: pandas dataframe con i bigrammi prefixes come righe,\n","                      le parole del vocabolario come colonne\n","                      e il conteggio delle combinazioni bigramma/parola come valore\n","    \"\"\"\n","    context_queue = (n - 1) * ['<s>']\n","    result = []\n","    for _ in range(token_count):\n","      if random_sampling:\n","        if tuple(context_queue) in prob_matrix.index.tolist():\n","          nonzero_probs = (prob_matrix.loc[[tuple(context_queue)]]> threshold_prob).any()\n","          tokens_list = nonzero_probs.index[nonzero_probs].tolist()\n","          obj = random.sample(tokens_list, 1)[0]\n","        else:\n","          return ' '.join(result)\n","      else:\n","        obj = prob_matrix.loc[[tuple(context_queue)]].max().idxmax()\n","      result.append(obj)\n","      if n > 1:\n","          context_queue.pop(0)\n","          if obj == '.':\n","              context_queue = (n - 1) * ['<s>']\n","          else:\n","              context_queue.append(obj)\n","    return ' '.join(result)\n"],"metadata":{"id":"BnsjB1EvJxrJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(10):\n","  print(generate_text(n, prob_matrix, 20, random_sampling=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGUzWAwChS-f","executionInfo":{"status":"ok","timestamp":1699478171818,"user_tz":-60,"elapsed":3,"user":{"displayName":"Luca Negri","userId":"08467063413129502685"}},"outputId":"19d7ca37-6568-4070-d830-1d4adce8079f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK\n","UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK\n","UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK\n","UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK\n","UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK\n","UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK\n","UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK\n","UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK\n","UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK\n","UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK . UNK UNK\n"]}]},{"cell_type":"code","source":["for i in range(10):\n","  print(generate_text(n, prob_matrix, 20, random_sampling=True, threshold_prob = 0.0000001))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vch3jqn2-gYf","executionInfo":{"status":"ok","timestamp":1699478172409,"user_tz":-60,"elapsed":264,"user":{"displayName":"Luca Negri","userId":"08467063413129502685"}},"outputId":"1a208a18-d260-4714-ae7b-c8ca8199f752"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["melittin </s>\n","which practitioner is more likely to get UNK in the conference center is strictly limited so UNK will be examined\n","obviously shell need a UNK to get anything and then i inject at about a 45 degree UNK it still\n","100 yrs ago it was done all the time at UNK but we are adding new information . tissue UNK\n","however its results were questionable at best since the researchers did not go on to measure whether the increase in\n","do some of you get a UNK UNK or internet UNK anonymous ftp UNK also UNK into windows UNK .\n","thats quite UNK since the UNK UNK for more than 30 years and finding it UNK to all UNK and\n","melittin </s>\n","nearly all hivinfected children UNK 47 new evidence that the hiv itself caused the skin disease . perhaps such pictures\n","those mice however became sick and died too soon after birth to study in depth . below is a press\n"]}]},{"cell_type":"code","source":["for i in range(10):\n","  print(generate_text(n, prob_matrix, 20, random_sampling=True, threshold_prob = 0.00005))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_ZFAzljZqgD","executionInfo":{"status":"ok","timestamp":1699478172642,"user_tz":-60,"elapsed":235,"user":{"displayName":"Luca Negri","userId":"08467063413129502685"}},"outputId":"1bf033b1-55a4-417f-c4f5-1be172c9d2e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["diseases such as aids . clinical research news high UNK assisted UNK technologies ......................... 24 4 . each child will\n","experiment and empirical studies are an important part of true scientific methodology is scientific thinking . profits of the group\n","glutamic acid is the most UNK UNK of UNK patients met the case definition of UNK e. coli and the\n","articles low levels UNK UNK linked to serious asthma attacks UNK 29 nih consensus development conference on UNK ..................... 31\n","free demo 5 outside us . despite the UNK findings scientists are UNK whether the same result occurs in the\n","how long ago was this ? </s>\n","younger age groups and the UNK of eligibility a couple of liters of corn much of it intact kernals .\n","hrsa is one of the a UNK . jb 2 were you able to understand my answer to your question\n","each child will receive six UNK every four weeks for six UNK be UNK for when necessary with glasses or\n","before that i used to get bloody noses nighttime asthma attacks and eyes so itchy i couldnt get to sleep\n"]}]},{"cell_type":"code","source":["for i in range(10):\n","  print(generate_text(n, prob_matrix, 20, random_sampling=True, threshold_prob = 0.00015))"],"metadata":{"id":"zrLthB0YbPuh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699478173136,"user_tz":-60,"elapsed":496,"user":{"displayName":"Luca Negri","userId":"08467063413129502685"}},"outputId":"3d38b9a1-bc80-406a-f2f1-ce2df94b8beb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["one or more of the other UNK made by genentech inc. of south san francisco and the other by biocine\n","div of field epidemiology epidemiology program office UNK diseases UNK div of health nevada state dept of human resources .\n","rutin is a UNK UNK in clinical trials ? </s>\n","when initially UNK up the software it lets you select several options 1 text graphics 2 UNK source ie scanner\n","later david </s>\n","worked UNK on windows UNK though it did occasionally UNK with a prize cure . even after i try to\n","shavlik 6 comparison of two approaches to the prediction of protein UNK k. UNK k. asai m. ishikawa a. UNK\n","eat a healthy balanced diet and relax . preliminary report foodborne outbreak of escherichia coli o157h7 other UNK e. coli\n","studies of tissue taken from the wartlike skin tumors showed that they were a type of UNK he is having\n","free demo 5 outside us . elevated levels of glu and UNK in six western states . having had limited\n"]}]},{"cell_type":"code","source":["for i in range(10):\n","  print(generate_text(n, prob_matrix, 20, random_sampling=True, threshold_prob = 0.002))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_in5I5XM-kpT","executionInfo":{"status":"ok","timestamp":1699478173491,"user_tz":-60,"elapsed":360,"user":{"displayName":"Luca Negri","userId":"08467063413129502685"}},"outputId":"6131627f-bbdb-49fa-bd70-cb5df3ca0c0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["it UNK my family no end since im at an UNK school sort of validated it for them ... then\n","one artificial sweetener that is not typically UNK of causing cancer is aspartame . 2 . jb 3 ron ...\n","good story UNK but it wont UNK . thank you UNK live from new york its saturday night ... </s>\n","this led them to UNK that the root cause of allergy may have a psychological UNK or basis . is\n","as growing numbers of women of UNK age become exposed to hiv through injection drug use or infected sexual partners\n","are there noninvasive ways of diagnosing a hernia ? </s>\n","UNK patrick ph.d. communications research UNK UNK canada UNK </s>\n","from article UNK by UNK the areas that are least likely to hurt are where you have a little gut\n","am j public health UNK . ive tried this when UNK and UNK ! </s>\n","so why bother with it ? </s>\n"]}]},{"cell_type":"code","source":["for i in range(10):\n","  print(generate_text(n, prob_matrix, 20, random_sampling=True, threshold_prob = 0.03))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40jJlTE7LQSD","executionInfo":{"status":"ok","timestamp":1699478173837,"user_tz":-60,"elapsed":354,"user":{"displayName":"Luca Negri","userId":"08467063413129502685"}},"outputId":"6116d7c3-733b-49e2-f2f6-b3b10004e1a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the UNK food UNK practice is to cook ground beef thoroughly until the interior is no longer UNK and the\n","the only problem is the personal dictionary will only handle about 200 words . UNK . UNK UNK UNK .\n","UNK UNK UNK UNK UNK and the food and drug administration have given aspartame one of the researchers who conducted\n","i am especially interested in getting reviews of these products from people who have UNK them or are using them\n","the UNK of certain health problems but also to be UNK to most calendar programs . the UNK of science\n","the only fungus i know of from california is UNK . UNK . i think 1 outlaw the use of\n","UNK UNK UNK UNK UNK and functional UNK . the only limits i know of from california is UNK .\n","the UNK who are UNK by drug resistance in conditions such as gonorrhea malaria pneumococcal disease UNK UNK tb and\n","i would love to have anyone come up with a study to support their claims that the placebo effect from\n","UNK UNK UNK and put it over the spot . i dont want to go into a lot of money\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"6GdKOkcwNf58"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1vigPPKTPiISLAlmvP24WBcoM16NE9Wtx","timestamp":1702919662811}],"authorship_tag":"ABX9TyML3kYstLnJh8ki1LsvvYn7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}